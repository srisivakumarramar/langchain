from langchain_ollama import OllamaEmbeddings, ChatOllama

from langchain_community.document_loaders import TextLoader

from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_chroma import Chroma

from langchain.prompts import ChatPromptTemplate

from langchain.chains import create_retrieval_chain

from langchain.chains.combine_documents import create_stuff_documents_chain





llm = ChatOllama(model="llama3.2")

embeddings=OllamaEmbeddings(model="llama3.2")





document = TextLoader("product-data.txt").load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,

                                              chunk_overlap=200)

chunks = text_splitter.split_documents(document)

vector_store = Chroma.from_documents(chunks,embeddings)

retriever = vector_store.as_retriever()



prompt_template = ChatPromptTemplate.from_messages(

[

    ("system","""You are an assistant for answering questions.

    Use the provided context to respond.If the answer

    isn't clear, acknowledge that you don't know.

    Limit your response to three concise sentences.

    {context}   

          """),

    ("human", "{input}")

]

)



qa_chain = create_stuff_documents_chain(llm, prompt_template)

rag_chain = create_retrieval_chain(retriever, qa_chain)



print("Chat with Document")

question=input("Your Question")



if question:

    response = rag_chain.invoke({"input":question})

    print(response['answer'])

Give Feedback

What went well? What could be improved?

SR
Add feedback...
Add feedback...
AS
Ashish Sinha
Posted 15 days ago
Instructions are in the video

from dotenv import load_dotenv
from langchain_ollama import OllamaEmbeddings
from langchain_openai import ChatOllama
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
#from langchain_ollama import OllamaEmbeddings
# from langchain.globals import set_debug

# set_debug(True)

load_dotenv()
embeddings = OllamaEmbeddings()
llm = ChatOllama(model="llama3.2:latest")

documents = TextLoader("data/product-data.txt").load()
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

vector_store = Chroma.from_documents(chunks, embeddings)
retriever = vector_store.as_retriever()
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", """
            You are an assistant for answering questions.
            Use the provided context to respond.If the answer
            isn't clear, acknowledge that you don't know.
            Limit your response to three concise sentences.
            {context}
         """
        ),
        ("human", "{input}"),
    ]
)
qa_chain = create_stuff_documents_chain(llm, prompt_template)
rag_chain = create_retrieval_chain(retriever, qa_chain)

print("Welcome to the RAG demo!")
question = input("Enter the question: ")
if question:
    response = rag_chain.invoke({"input": question})
    print(response["answer"])
    
